{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis - Part 3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenjaminPhillips22/NLP-sentiment-analysis/blob/master/Sentiment_Analysis_Part_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "0Pt-3prp7YGT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis - Pre-Trained Embeddings, LSTM\n",
        "\n",
        "This Notebook trains a LSTM neural network to determine if a movie review is positive or negative. Pretrained work embeddings with 100 dimensions are used. The input to the network is the sequence of word embeddings of the review."
      ]
    },
    {
      "metadata": {
        "id": "S8ihkFkcQH18",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install torchtext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "clEVpVCkNv6a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext import vocab\n",
        "from torch import nn\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import gensim.downloader as api\n",
        "\n",
        "import spacy\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iTJzu9gaNhAW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "SEED = 1912\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize='spacy')\n",
        "LABEL = data.LabelField(dtype=torch.float)\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "train_data, dev_data = train_data.split(random_state=random.seed(SEED))\n",
        "\n",
        "imdb_data = {'train': train_data, 'test': test_data, 'dev': dev_data }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rVrTMknKsBOq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors from gensim-data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yhm5KgOQ6foa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7uhDwJeD6hUl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now I want to make a function to take a training example and output the average of the word embeddings"
      ]
    },
    {
      "metadata": {
        "id": "ZFRKlyr26gG0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def word_check(word, word_vectors):\n",
        "    word = word.lower()\n",
        "    word = re.sub(r'[^A-Za-z0-9]+', '', word) # remove non alphanumeric character\n",
        "    word = re.sub(r'https?:/\\/\\S+', '', word) # remove links\n",
        "    if word in word_vectors.vocab:\n",
        "        return True, word_vectors[word]\n",
        "    return False, None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8tkIyiK36gY0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sentence_embedding(sentence, word_vectors):\n",
        "    \"\"\" sentence is a list of words\"\"\"\n",
        "    \n",
        "    embeds = []\n",
        "    for word in sentence:\n",
        "        boo, woo = word_check(word, word_vectors)\n",
        "        if boo:\n",
        "            embeds.append(woo)\n",
        "    \n",
        "    # no suitable words\n",
        "    if len(embeds)==0:\n",
        "        return torch.Tensor([np.zeros(shape=word_vectors['a'].shape)])\n",
        "    return torch.Tensor(embeds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lMWLSAnaEuCh",
        "colab_type": "code",
        "outputId": "cbca014a-8459-4440-db31-8d9bd0f54061",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "keys = ['train', 'test', 'dev']\n",
        "\n",
        "imdb_data_sentence = {}\n",
        "\n",
        "for k in keys:\n",
        "    print(k)\n",
        "    data_words = []\n",
        "    data_label = []\n",
        "    for ex in imdb_data[k].examples:\n",
        "        data_words.append(sentence_embedding(ex.text, word_vectors))\n",
        "        if ex.label == 'pos':\n",
        "            l = 1\n",
        "        elif ex.label == 'neg':\n",
        "            l = 0\n",
        "        else:\n",
        "            raise ValueError('unexpected value for label')\n",
        "\n",
        "        data_label.append(l)\n",
        "    \n",
        "    imdb_data_sentence[k] = {'sentences': data_words, 'labels': data_label}\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "test\n",
            "dev\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DoCtzq0gUjwG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Time to make a model!!!"
      ]
    },
    {
      "metadata": {
        "id": "Wgl5lSWlCBtN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Model thanks to http://www.jessicayung.com/lstms-for-time-series-in-pytorch/\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1, num_layers=2):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Define the LSTM layer\n",
        "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
        "\n",
        "        # Define the output layer\n",
        "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # This is what we'll initialise our hidden state as\n",
        "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
        "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Forward pass through LSTM layer\n",
        "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
        "        # shape of self.hidden: (a, b), where a and b both \n",
        "        # have shape (num_layers, batch_size, hidden_dim).\n",
        "        lstm_out, self.hidden = self.lstm(input.view(len(input), self.batch_size, -1))\n",
        "        \n",
        "        # Only take the output from the final timetep\n",
        "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
        "        y_pred = torch.nn.functional.sigmoid(self.linear(lstm_out[-1].view(self.batch_size, -1)))\n",
        "        return y_pred.view(-1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KAKGBtWKJi0i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def my_auc(y, pred):\n",
        "    from sklearn.metrics import auc\n",
        "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(y, pred)\n",
        "    return sklearn.metrics.auc(fpr, tpr)\n",
        "\n",
        "\n",
        "def get_dataset_auc(dataset, model, sample_size=100):\n",
        "    # dataset too big to run all of the examples\n",
        "    our_steps = np.random.choice(range(len(dataset['sentences'])), sample_size, replace=False)\n",
        "    pred = []\n",
        "    for step in our_steps:\n",
        "        pred.append(model(dataset['sentences'][step].cuda()).detach().cpu().numpy())\n",
        "    labels = [dataset['labels'][step] for step in our_steps]\n",
        "    return my_auc(labels, pred)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1_YSsP4OL5_I",
        "colab_type": "code",
        "outputId": "b2621343-d1a7-40ce-ad92-36f3f7c03d01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16064
        }
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "NUM_BATCHES = np.int(np.floor(len(imdb_data_sentence['train']['sentences'])/BATCH_SIZE))\n",
        "EPOCH = 17\n",
        "\n",
        "auc_sample_size = BATCH_SIZE*4\n",
        "\n",
        "net = LSTM(input_dim=100, hidden_dim=10, batch_size=1, output_dim=1, num_layers=1).cuda()\n",
        "\n",
        "optimizer = torch.optim.Adam(net.parameters())\n",
        "criterion = torch.nn.BCEWithLogitsLoss().cuda()\n",
        "\n",
        "train_set_auc = []\n",
        "dev_set_auc = []\n",
        "training_loss = []\n",
        "\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    \n",
        "    num_training_examples = len(imdb_data_sentence['train']['sentences'])\n",
        "    our_steps = np.random.choice(range(num_training_examples), num_training_examples, replace=False)\n",
        "    step_index = 0\n",
        "    \n",
        "    for b in range(NUM_BATCHES):\n",
        "      \n",
        "        predictions = torch.Tensor().cuda()\n",
        "        labels = torch.Tensor().cuda()\n",
        "          \n",
        "        for _ in range(BATCH_SIZE):  \n",
        "            \n",
        "            step = our_steps[step_index]\n",
        "            step_index += 1\n",
        "            \n",
        "            text = imdb_data_sentence['train']['sentences'][step].cuda()\n",
        "            l = torch.Tensor([imdb_data_sentence['train']['labels'][step]]).cuda()\n",
        "\n",
        "            net.hidden = net.init_hidden()\n",
        "            pred = net(text)                   # input x and predict based on x\n",
        "            loss = criterion(predictions, labels)\n",
        "            training_loss.append(loss)\n",
        "            \n",
        "            predictions = torch.cat((predictions, pred), 0)\n",
        "            labels = torch.cat((labels, l), 0)\n",
        "       \n",
        "\n",
        "        loss = criterion(predictions, labels)     # must be (1. nn output, 2. target)\n",
        "        if b % 5 == 0:\n",
        "            print('epoch: ', epoch, ' batch: ', b, ' loss: ', loss)\n",
        "            \n",
        "            # save and load models\n",
        "            # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
        "            # torch.save(net.state_dict(), 'pretrained_embeddings_lstm_' + str(epoch) + '_batch_' + str(b))\n",
        "        \n",
        "        loss.backward()         # backpropagation, compute gradients\n",
        "\n",
        "        optimizer.step()        # apply gradients\n",
        "        \n",
        "        optimizer.zero_grad()   # clear gradients for next train\n",
        "        \n",
        "        \n",
        "\n",
        "    # every epoche, get the training data auc and dev data auc\n",
        "    tsa = get_dataset_auc(imdb_data_sentence['train'], net, auc_sample_size)\n",
        "    dsa = get_dataset_auc(imdb_data_sentence['dev'], net, auc_sample_size)\n",
        "    print('epoch: ', str(epoch), ' trainset auc: ', round(tsa, 2), ' dev set auc: ', round(dsa, 2))\n",
        "    train_set_auc.append(tsa)\n",
        "    dev_set_auc.append(dsa)\n",
        "    \n",
        "    # save and load models\n",
        "    # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
        "    torch.save(net.state_dict(), 'pretrained_embeddings_lstm_' + str(epoch))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch:  0  batch:  0  loss:  tensor(0.6889, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  5  loss:  tensor(0.6892, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  10  loss:  tensor(0.6756, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  15  loss:  tensor(0.7063, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  20  loss:  tensor(0.7404, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  25  loss:  tensor(0.6681, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  30  loss:  tensor(0.6985, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  35  loss:  tensor(0.7195, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  40  loss:  tensor(0.7191, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  45  loss:  tensor(0.7075, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  50  loss:  tensor(0.7297, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  55  loss:  tensor(0.6815, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  60  loss:  tensor(0.6842, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  65  loss:  tensor(0.6770, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  70  loss:  tensor(0.7048, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  75  loss:  tensor(0.7148, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  80  loss:  tensor(0.7001, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  85  loss:  tensor(0.7028, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  90  loss:  tensor(0.6966, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  95  loss:  tensor(0.7092, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  100  loss:  tensor(0.6935, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  105  loss:  tensor(0.6956, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  110  loss:  tensor(0.7048, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  115  loss:  tensor(0.7029, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  120  loss:  tensor(0.6952, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  125  loss:  tensor(0.6811, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  130  loss:  tensor(0.6912, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  135  loss:  tensor(0.7055, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  140  loss:  tensor(0.6846, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  145  loss:  tensor(0.6920, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  150  loss:  tensor(0.6803, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  155  loss:  tensor(0.6965, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  160  loss:  tensor(0.7055, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  165  loss:  tensor(0.6981, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  170  loss:  tensor(0.7086, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  175  loss:  tensor(0.6948, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  180  loss:  tensor(0.6890, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  185  loss:  tensor(0.7151, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  190  loss:  tensor(0.6901, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  195  loss:  tensor(0.6936, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  200  loss:  tensor(0.6965, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  205  loss:  tensor(0.6974, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  210  loss:  tensor(0.7052, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  215  loss:  tensor(0.6851, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  220  loss:  tensor(0.6975, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  225  loss:  tensor(0.6917, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  230  loss:  tensor(0.6932, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  235  loss:  tensor(0.6781, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  240  loss:  tensor(0.6660, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  245  loss:  tensor(0.6890, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  250  loss:  tensor(0.6801, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  255  loss:  tensor(0.6928, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  260  loss:  tensor(0.6757, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  265  loss:  tensor(0.6639, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  batch:  270  loss:  tensor(0.6648, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  0  trainset auc:  0.69  dev set auc:  0.74\n",
            "epoch:  1  batch:  0  loss:  tensor(0.6237, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  5  loss:  tensor(0.6710, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  10  loss:  tensor(0.6650, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  15  loss:  tensor(0.6757, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  20  loss:  tensor(0.6711, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  25  loss:  tensor(0.6663, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  30  loss:  tensor(0.6852, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  35  loss:  tensor(0.6904, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  40  loss:  tensor(0.6877, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  45  loss:  tensor(0.6856, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  50  loss:  tensor(0.6760, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  55  loss:  tensor(0.6573, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  60  loss:  tensor(0.6760, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  65  loss:  tensor(0.7080, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  70  loss:  tensor(0.7001, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  75  loss:  tensor(0.6693, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  80  loss:  tensor(0.6698, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  85  loss:  tensor(0.6712, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  90  loss:  tensor(0.6901, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  95  loss:  tensor(0.6664, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  100  loss:  tensor(0.6303, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  105  loss:  tensor(0.6493, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  110  loss:  tensor(0.6638, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  115  loss:  tensor(0.6286, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  120  loss:  tensor(0.6510, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  125  loss:  tensor(0.6616, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  130  loss:  tensor(0.6873, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  135  loss:  tensor(0.6845, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  140  loss:  tensor(0.6639, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  145  loss:  tensor(0.6520, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  150  loss:  tensor(0.6422, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  155  loss:  tensor(0.6883, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  160  loss:  tensor(0.6861, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  165  loss:  tensor(0.6669, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  170  loss:  tensor(0.6419, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  175  loss:  tensor(0.6556, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  180  loss:  tensor(0.6337, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  185  loss:  tensor(0.6305, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  190  loss:  tensor(0.6409, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  195  loss:  tensor(0.6503, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  200  loss:  tensor(0.6588, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  205  loss:  tensor(0.6885, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  210  loss:  tensor(0.6916, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  215  loss:  tensor(0.6773, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  220  loss:  tensor(0.6845, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  225  loss:  tensor(0.6903, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  230  loss:  tensor(0.6859, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  235  loss:  tensor(0.6896, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  240  loss:  tensor(0.6795, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  245  loss:  tensor(0.6874, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  250  loss:  tensor(0.6811, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  255  loss:  tensor(0.6889, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  260  loss:  tensor(0.6701, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  265  loss:  tensor(0.6634, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  batch:  270  loss:  tensor(0.6639, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  1  trainset auc:  0.66  dev set auc:  0.6\n",
            "epoch:  2  batch:  0  loss:  tensor(0.6683, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  5  loss:  tensor(0.6508, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  10  loss:  tensor(0.6585, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  15  loss:  tensor(0.6672, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  20  loss:  tensor(0.6505, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  25  loss:  tensor(0.6330, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  30  loss:  tensor(0.6549, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  35  loss:  tensor(0.6707, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  40  loss:  tensor(0.6777, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  45  loss:  tensor(0.6545, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  50  loss:  tensor(0.6798, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  55  loss:  tensor(0.6638, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  60  loss:  tensor(0.6971, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  65  loss:  tensor(0.6523, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  70  loss:  tensor(0.6900, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  75  loss:  tensor(0.6766, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  80  loss:  tensor(0.6718, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  85  loss:  tensor(0.6748, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  90  loss:  tensor(0.6864, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  95  loss:  tensor(0.6791, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  100  loss:  tensor(0.6744, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  105  loss:  tensor(0.6554, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  110  loss:  tensor(0.6621, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  115  loss:  tensor(0.6644, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  120  loss:  tensor(0.6577, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  125  loss:  tensor(0.6569, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  130  loss:  tensor(0.6823, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  135  loss:  tensor(0.6885, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  140  loss:  tensor(0.6449, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  145  loss:  tensor(0.6225, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  150  loss:  tensor(0.6019, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  155  loss:  tensor(0.6820, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  160  loss:  tensor(0.6471, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  165  loss:  tensor(0.6398, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  170  loss:  tensor(0.6572, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  175  loss:  tensor(0.6613, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  180  loss:  tensor(0.6429, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  185  loss:  tensor(0.6630, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  190  loss:  tensor(0.6179, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  195  loss:  tensor(0.5819, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  200  loss:  tensor(0.6283, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  205  loss:  tensor(0.5911, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  210  loss:  tensor(0.6432, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  215  loss:  tensor(0.6241, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  220  loss:  tensor(0.7492, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  225  loss:  tensor(0.7845, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  230  loss:  tensor(0.8295, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  235  loss:  tensor(0.7130, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  240  loss:  tensor(0.8799, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  245  loss:  tensor(0.8069, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  250  loss:  tensor(0.7062, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  255  loss:  tensor(0.7451, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  260  loss:  tensor(0.7147, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  265  loss:  tensor(0.8052, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  batch:  270  loss:  tensor(0.7892, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  2  trainset auc:  0.7  dev set auc:  0.64\n",
            "epoch:  3  batch:  0  loss:  tensor(0.6868, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  5  loss:  tensor(0.7133, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  10  loss:  tensor(0.6540, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  15  loss:  tensor(0.6887, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  20  loss:  tensor(0.7199, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  25  loss:  tensor(0.6361, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  30  loss:  tensor(0.6710, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  35  loss:  tensor(0.6421, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  40  loss:  tensor(0.7268, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  45  loss:  tensor(0.6177, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  50  loss:  tensor(0.6820, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  55  loss:  tensor(0.6158, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  60  loss:  tensor(0.6433, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  65  loss:  tensor(0.6316, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  70  loss:  tensor(0.6648, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  75  loss:  tensor(0.6057, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  80  loss:  tensor(0.6612, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  85  loss:  tensor(0.6153, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  90  loss:  tensor(0.6679, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  95  loss:  tensor(0.6902, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  100  loss:  tensor(0.6217, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  105  loss:  tensor(0.6362, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  110  loss:  tensor(0.6603, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  115  loss:  tensor(0.6639, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  120  loss:  tensor(0.6557, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  125  loss:  tensor(0.6575, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  130  loss:  tensor(0.6206, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  135  loss:  tensor(0.6029, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  140  loss:  tensor(0.6551, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  145  loss:  tensor(0.6669, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  150  loss:  tensor(0.6715, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  155  loss:  tensor(0.6486, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  160  loss:  tensor(0.6409, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  165  loss:  tensor(0.6422, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  170  loss:  tensor(0.6402, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  175  loss:  tensor(0.6525, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  180  loss:  tensor(0.6680, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  185  loss:  tensor(0.5932, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  190  loss:  tensor(0.6547, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  195  loss:  tensor(0.6538, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  200  loss:  tensor(0.6408, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  205  loss:  tensor(0.5955, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  210  loss:  tensor(0.6287, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  215  loss:  tensor(0.6530, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  220  loss:  tensor(0.6517, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  225  loss:  tensor(0.6378, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  230  loss:  tensor(0.6855, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  235  loss:  tensor(0.6354, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  240  loss:  tensor(0.6278, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  245  loss:  tensor(0.6142, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  250  loss:  tensor(0.6010, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  255  loss:  tensor(0.5980, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  260  loss:  tensor(0.6602, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  265  loss:  tensor(0.6496, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  batch:  270  loss:  tensor(0.6409, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  3  trainset auc:  0.83  dev set auc:  0.78\n",
            "epoch:  4  batch:  0  loss:  tensor(0.6616, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  5  loss:  tensor(0.5945, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  10  loss:  tensor(0.6833, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  15  loss:  tensor(0.6298, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  20  loss:  tensor(0.6234, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  25  loss:  tensor(0.6248, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  30  loss:  tensor(0.6510, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  35  loss:  tensor(0.6336, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  40  loss:  tensor(0.5919, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  45  loss:  tensor(0.6148, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  50  loss:  tensor(0.6376, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  55  loss:  tensor(0.6251, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  60  loss:  tensor(0.6161, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  65  loss:  tensor(0.6172, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  70  loss:  tensor(0.6730, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  75  loss:  tensor(0.6568, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  80  loss:  tensor(0.6029, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  85  loss:  tensor(0.6476, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  90  loss:  tensor(0.5737, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  95  loss:  tensor(0.6199, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  100  loss:  tensor(0.6280, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  105  loss:  tensor(0.6222, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  110  loss:  tensor(0.6297, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  115  loss:  tensor(0.6172, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  120  loss:  tensor(0.6253, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  125  loss:  tensor(0.6719, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  130  loss:  tensor(0.6298, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  135  loss:  tensor(0.6725, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  140  loss:  tensor(0.6316, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  145  loss:  tensor(0.6349, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  150  loss:  tensor(0.6431, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  155  loss:  tensor(0.6675, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  160  loss:  tensor(0.6052, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  165  loss:  tensor(0.6635, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  170  loss:  tensor(0.5923, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  175  loss:  tensor(0.6341, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  180  loss:  tensor(0.6184, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  185  loss:  tensor(0.6375, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  190  loss:  tensor(0.5941, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  195  loss:  tensor(0.6472, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  200  loss:  tensor(0.5442, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  205  loss:  tensor(0.6417, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  210  loss:  tensor(0.6353, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  215  loss:  tensor(0.5723, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  220  loss:  tensor(0.5913, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  225  loss:  tensor(0.6008, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  230  loss:  tensor(0.6502, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  235  loss:  tensor(0.6251, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  240  loss:  tensor(0.6404, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  245  loss:  tensor(0.6364, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  250  loss:  tensor(0.6171, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  255  loss:  tensor(0.6551, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  260  loss:  tensor(0.6456, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  265  loss:  tensor(0.6221, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  batch:  270  loss:  tensor(0.6362, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  4  trainset auc:  0.82  dev set auc:  0.86\n",
            "epoch:  5  batch:  0  loss:  tensor(0.6571, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  5  loss:  tensor(0.6182, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  10  loss:  tensor(0.6547, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  15  loss:  tensor(0.6456, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  20  loss:  tensor(0.6198, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  25  loss:  tensor(0.6250, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  30  loss:  tensor(0.5820, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  35  loss:  tensor(0.6164, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  40  loss:  tensor(0.6401, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  45  loss:  tensor(0.6237, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  50  loss:  tensor(0.6305, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  55  loss:  tensor(0.6055, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  60  loss:  tensor(0.6630, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  65  loss:  tensor(0.6423, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  70  loss:  tensor(0.5767, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  75  loss:  tensor(0.6238, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  80  loss:  tensor(0.6259, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  85  loss:  tensor(0.6463, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  90  loss:  tensor(0.6304, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  95  loss:  tensor(0.6240, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  100  loss:  tensor(0.5927, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  105  loss:  tensor(0.6283, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  110  loss:  tensor(0.6555, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  115  loss:  tensor(0.6022, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  120  loss:  tensor(0.5863, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  125  loss:  tensor(0.6330, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  130  loss:  tensor(0.5774, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  135  loss:  tensor(0.5879, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  140  loss:  tensor(0.6292, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  145  loss:  tensor(0.6196, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  150  loss:  tensor(0.5955, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  155  loss:  tensor(0.6591, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  160  loss:  tensor(0.6337, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  165  loss:  tensor(0.6163, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  170  loss:  tensor(0.6169, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  175  loss:  tensor(0.6038, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  180  loss:  tensor(0.5721, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  185  loss:  tensor(0.5697, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  190  loss:  tensor(0.5814, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  195  loss:  tensor(0.5720, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  200  loss:  tensor(0.6330, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  205  loss:  tensor(0.6012, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  210  loss:  tensor(0.5906, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  215  loss:  tensor(0.6058, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  220  loss:  tensor(0.6824, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  225  loss:  tensor(0.6802, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  230  loss:  tensor(0.6322, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  235  loss:  tensor(0.6002, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  240  loss:  tensor(0.6199, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  245  loss:  tensor(0.6389, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  250  loss:  tensor(0.6085, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  255  loss:  tensor(0.6188, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  260  loss:  tensor(0.6406, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  265  loss:  tensor(0.5929, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  batch:  270  loss:  tensor(0.6305, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  5  trainset auc:  0.86  dev set auc:  0.81\n",
            "epoch:  6  batch:  0  loss:  tensor(0.6200, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  5  loss:  tensor(0.5982, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  10  loss:  tensor(0.5763, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  15  loss:  tensor(0.6339, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  20  loss:  tensor(0.6053, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  25  loss:  tensor(0.6672, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  30  loss:  tensor(0.6007, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  35  loss:  tensor(0.6083, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  40  loss:  tensor(0.6653, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  45  loss:  tensor(0.6261, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  50  loss:  tensor(0.6381, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  55  loss:  tensor(0.5893, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  60  loss:  tensor(0.5796, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  65  loss:  tensor(0.6183, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  70  loss:  tensor(0.6259, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  75  loss:  tensor(0.6065, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  80  loss:  tensor(0.5685, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  85  loss:  tensor(0.5947, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  90  loss:  tensor(0.6366, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  95  loss:  tensor(0.5941, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  100  loss:  tensor(0.5878, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  105  loss:  tensor(0.5983, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  110  loss:  tensor(0.6263, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  115  loss:  tensor(0.6098, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  120  loss:  tensor(0.6641, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  125  loss:  tensor(0.6167, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  130  loss:  tensor(0.6331, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  135  loss:  tensor(0.6332, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  140  loss:  tensor(0.6096, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  145  loss:  tensor(0.6258, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  150  loss:  tensor(0.6548, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  155  loss:  tensor(0.6110, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  160  loss:  tensor(0.6021, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  165  loss:  tensor(0.6046, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  170  loss:  tensor(0.6049, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  175  loss:  tensor(0.5961, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  180  loss:  tensor(0.5685, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  185  loss:  tensor(0.5890, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  190  loss:  tensor(0.6363, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  195  loss:  tensor(0.5997, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  200  loss:  tensor(0.5931, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  205  loss:  tensor(0.5946, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  210  loss:  tensor(0.6546, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  215  loss:  tensor(0.6006, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  220  loss:  tensor(0.6553, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  225  loss:  tensor(0.5881, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  230  loss:  tensor(0.6116, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  235  loss:  tensor(0.6022, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  240  loss:  tensor(0.6058, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  245  loss:  tensor(0.6345, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  250  loss:  tensor(0.6345, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  255  loss:  tensor(0.6125, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  260  loss:  tensor(0.6412, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  265  loss:  tensor(0.6210, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  batch:  270  loss:  tensor(0.5820, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  6  trainset auc:  0.87  dev set auc:  0.84\n",
            "epoch:  7  batch:  0  loss:  tensor(0.6176, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  5  loss:  tensor(0.5854, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  10  loss:  tensor(0.6170, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  15  loss:  tensor(0.5558, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  20  loss:  tensor(0.5508, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  25  loss:  tensor(0.5965, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  30  loss:  tensor(0.5938, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  35  loss:  tensor(0.5892, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  40  loss:  tensor(0.5738, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  45  loss:  tensor(0.5724, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  50  loss:  tensor(0.6004, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  55  loss:  tensor(0.5609, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  60  loss:  tensor(0.6433, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  65  loss:  tensor(0.5892, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  70  loss:  tensor(0.6044, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  75  loss:  tensor(0.6071, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  80  loss:  tensor(0.6189, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  85  loss:  tensor(0.6471, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  90  loss:  tensor(0.5836, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  95  loss:  tensor(0.5843, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  100  loss:  tensor(0.6533, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  105  loss:  tensor(0.6024, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  110  loss:  tensor(0.5933, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  115  loss:  tensor(0.5644, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  120  loss:  tensor(0.6308, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  125  loss:  tensor(0.6136, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  130  loss:  tensor(0.6204, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  135  loss:  tensor(0.5649, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  140  loss:  tensor(0.6040, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  145  loss:  tensor(0.6444, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  150  loss:  tensor(0.6114, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  155  loss:  tensor(0.6348, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  160  loss:  tensor(0.5983, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  165  loss:  tensor(0.6230, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  170  loss:  tensor(0.6300, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  175  loss:  tensor(0.6070, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  180  loss:  tensor(0.6016, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  185  loss:  tensor(0.6245, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  190  loss:  tensor(0.5763, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  195  loss:  tensor(0.5976, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  200  loss:  tensor(0.6074, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  205  loss:  tensor(0.5461, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  210  loss:  tensor(0.6064, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  215  loss:  tensor(0.6065, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  220  loss:  tensor(0.5905, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  225  loss:  tensor(0.6083, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  230  loss:  tensor(0.5609, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  235  loss:  tensor(0.5991, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  240  loss:  tensor(0.5871, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  245  loss:  tensor(0.6382, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  250  loss:  tensor(0.5841, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  255  loss:  tensor(0.6601, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  260  loss:  tensor(0.5338, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  265  loss:  tensor(0.6215, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  batch:  270  loss:  tensor(0.5693, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  7  trainset auc:  0.83  dev set auc:  0.86\n",
            "epoch:  8  batch:  0  loss:  tensor(0.6301, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  5  loss:  tensor(0.5942, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  10  loss:  tensor(0.6403, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  15  loss:  tensor(0.6112, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  20  loss:  tensor(0.5501, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  25  loss:  tensor(0.6119, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  30  loss:  tensor(0.6398, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  35  loss:  tensor(0.5826, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  40  loss:  tensor(0.6122, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  45  loss:  tensor(0.5709, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  50  loss:  tensor(0.5598, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  55  loss:  tensor(0.6065, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  60  loss:  tensor(0.5772, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  65  loss:  tensor(0.5626, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  70  loss:  tensor(0.5819, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  75  loss:  tensor(0.5984, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  80  loss:  tensor(0.6303, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  85  loss:  tensor(0.5980, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  90  loss:  tensor(0.6037, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  95  loss:  tensor(0.5758, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  100  loss:  tensor(0.6108, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  105  loss:  tensor(0.6316, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  110  loss:  tensor(0.5630, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  115  loss:  tensor(0.6078, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  120  loss:  tensor(0.5220, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  125  loss:  tensor(0.5559, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  130  loss:  tensor(0.5408, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  135  loss:  tensor(0.6372, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  140  loss:  tensor(0.5736, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  145  loss:  tensor(0.6100, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  150  loss:  tensor(0.5977, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  155  loss:  tensor(0.6351, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  160  loss:  tensor(0.6054, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  165  loss:  tensor(0.5694, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  170  loss:  tensor(0.5658, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  175  loss:  tensor(0.6013, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  180  loss:  tensor(0.6032, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  185  loss:  tensor(0.6284, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  190  loss:  tensor(0.5965, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  195  loss:  tensor(0.5564, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  200  loss:  tensor(0.6265, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  205  loss:  tensor(0.5731, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  210  loss:  tensor(0.5849, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  215  loss:  tensor(0.6377, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  220  loss:  tensor(0.6246, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  225  loss:  tensor(0.5939, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  230  loss:  tensor(0.6083, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  235  loss:  tensor(0.6152, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  240  loss:  tensor(0.6129, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  245  loss:  tensor(0.5507, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  250  loss:  tensor(0.6230, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  255  loss:  tensor(0.5821, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  260  loss:  tensor(0.6069, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  265  loss:  tensor(0.5503, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  batch:  270  loss:  tensor(0.6598, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  8  trainset auc:  0.87  dev set auc:  0.85\n",
            "epoch:  9  batch:  0  loss:  tensor(0.5962, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  5  loss:  tensor(0.5851, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  10  loss:  tensor(0.5746, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  15  loss:  tensor(0.5940, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  20  loss:  tensor(0.5425, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  25  loss:  tensor(0.5468, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  30  loss:  tensor(0.6157, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  35  loss:  tensor(0.5697, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  40  loss:  tensor(0.6031, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  45  loss:  tensor(0.5760, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  50  loss:  tensor(0.5775, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  55  loss:  tensor(0.6151, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  60  loss:  tensor(0.6363, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  65  loss:  tensor(0.5838, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  70  loss:  tensor(0.6048, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  75  loss:  tensor(0.6171, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  80  loss:  tensor(0.6457, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  85  loss:  tensor(0.5794, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  90  loss:  tensor(0.5551, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  95  loss:  tensor(0.5503, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  100  loss:  tensor(0.5982, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  105  loss:  tensor(0.5822, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  110  loss:  tensor(0.6059, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  115  loss:  tensor(0.6011, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  120  loss:  tensor(0.6054, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  125  loss:  tensor(0.5621, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  130  loss:  tensor(0.6005, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  135  loss:  tensor(0.5609, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  140  loss:  tensor(0.5967, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  145  loss:  tensor(0.5535, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  150  loss:  tensor(0.6008, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  155  loss:  tensor(0.5806, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  160  loss:  tensor(0.6359, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  165  loss:  tensor(0.5757, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  170  loss:  tensor(0.5836, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  175  loss:  tensor(0.5970, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  180  loss:  tensor(0.6143, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  185  loss:  tensor(0.5949, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  190  loss:  tensor(0.5845, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  195  loss:  tensor(0.6180, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  200  loss:  tensor(0.6310, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  205  loss:  tensor(0.5837, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  210  loss:  tensor(0.6027, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  215  loss:  tensor(0.6051, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  220  loss:  tensor(0.5768, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  225  loss:  tensor(0.5481, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  230  loss:  tensor(0.6337, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  235  loss:  tensor(0.5559, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  240  loss:  tensor(0.6333, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  245  loss:  tensor(0.5714, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  250  loss:  tensor(0.6078, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  255  loss:  tensor(0.5494, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  260  loss:  tensor(0.6096, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  265  loss:  tensor(0.5846, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  batch:  270  loss:  tensor(0.6096, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  9  trainset auc:  0.85  dev set auc:  0.88\n",
            "epoch:  10  batch:  0  loss:  tensor(0.6600, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  5  loss:  tensor(0.6127, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  10  loss:  tensor(0.5796, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  15  loss:  tensor(0.6070, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  20  loss:  tensor(0.5475, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  25  loss:  tensor(0.5931, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  30  loss:  tensor(0.6111, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  35  loss:  tensor(0.6271, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  40  loss:  tensor(0.6033, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  45  loss:  tensor(0.6275, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  50  loss:  tensor(0.6220, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  55  loss:  tensor(0.5956, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  60  loss:  tensor(0.5799, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  65  loss:  tensor(0.5638, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  70  loss:  tensor(0.5848, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  75  loss:  tensor(0.5634, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  80  loss:  tensor(0.6312, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  85  loss:  tensor(0.5757, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  90  loss:  tensor(0.6088, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  95  loss:  tensor(0.5896, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  100  loss:  tensor(0.6140, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  105  loss:  tensor(0.5969, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  110  loss:  tensor(0.6185, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  115  loss:  tensor(0.5742, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  120  loss:  tensor(0.5468, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  125  loss:  tensor(0.6473, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  130  loss:  tensor(0.5601, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  135  loss:  tensor(0.6113, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  140  loss:  tensor(0.6020, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  145  loss:  tensor(0.5549, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  150  loss:  tensor(0.6065, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  155  loss:  tensor(0.6119, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  160  loss:  tensor(0.5719, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  165  loss:  tensor(0.6067, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  170  loss:  tensor(0.5624, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  175  loss:  tensor(0.6070, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  180  loss:  tensor(0.5838, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  185  loss:  tensor(0.5864, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  190  loss:  tensor(0.5827, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  195  loss:  tensor(0.6155, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  200  loss:  tensor(0.5944, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  205  loss:  tensor(0.5729, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  210  loss:  tensor(0.5972, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  215  loss:  tensor(0.5712, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  220  loss:  tensor(0.5831, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  225  loss:  tensor(0.5750, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  230  loss:  tensor(0.5823, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  235  loss:  tensor(0.6072, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  240  loss:  tensor(0.6222, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  245  loss:  tensor(0.6109, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  250  loss:  tensor(0.6557, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  255  loss:  tensor(0.5716, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  260  loss:  tensor(0.6443, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  265  loss:  tensor(0.5783, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  batch:  270  loss:  tensor(0.5690, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  10  trainset auc:  0.9  dev set auc:  0.88\n",
            "epoch:  11  batch:  0  loss:  tensor(0.5874, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  5  loss:  tensor(0.5522, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  10  loss:  tensor(0.5454, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  15  loss:  tensor(0.6340, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  20  loss:  tensor(0.6009, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  25  loss:  tensor(0.6238, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  30  loss:  tensor(0.5220, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  35  loss:  tensor(0.6035, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  40  loss:  tensor(0.5727, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  45  loss:  tensor(0.5703, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  50  loss:  tensor(0.6179, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  55  loss:  tensor(0.6337, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  60  loss:  tensor(0.6164, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  65  loss:  tensor(0.5721, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  70  loss:  tensor(0.6172, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  75  loss:  tensor(0.5672, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  80  loss:  tensor(0.5561, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  85  loss:  tensor(0.5656, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  90  loss:  tensor(0.6209, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  95  loss:  tensor(0.5355, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  100  loss:  tensor(0.5690, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  105  loss:  tensor(0.5733, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  110  loss:  tensor(0.6200, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  115  loss:  tensor(0.5873, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  120  loss:  tensor(0.6661, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  125  loss:  tensor(0.5399, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  130  loss:  tensor(0.5817, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  135  loss:  tensor(0.6041, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  140  loss:  tensor(0.5945, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  145  loss:  tensor(0.6144, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  150  loss:  tensor(0.5446, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  155  loss:  tensor(0.6062, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  160  loss:  tensor(0.5661, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  165  loss:  tensor(0.5936, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  170  loss:  tensor(0.5946, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  175  loss:  tensor(0.6011, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  180  loss:  tensor(0.5831, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  185  loss:  tensor(0.6263, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  190  loss:  tensor(0.5937, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  195  loss:  tensor(0.5913, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  200  loss:  tensor(0.5999, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  205  loss:  tensor(0.5647, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  210  loss:  tensor(0.5863, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  215  loss:  tensor(0.5747, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  220  loss:  tensor(0.6583, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  225  loss:  tensor(0.6133, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  230  loss:  tensor(0.5256, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  235  loss:  tensor(0.6095, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  240  loss:  tensor(0.6026, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  245  loss:  tensor(0.6135, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  250  loss:  tensor(0.5952, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  255  loss:  tensor(0.5916, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  260  loss:  tensor(0.5650, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  265  loss:  tensor(0.5370, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  batch:  270  loss:  tensor(0.5743, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  11  trainset auc:  0.88  dev set auc:  0.89\n",
            "epoch:  12  batch:  0  loss:  tensor(0.5395, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  5  loss:  tensor(0.5595, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  10  loss:  tensor(0.5605, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  15  loss:  tensor(0.5921, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  20  loss:  tensor(0.6121, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  25  loss:  tensor(0.5730, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  30  loss:  tensor(0.6377, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  35  loss:  tensor(0.6020, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  40  loss:  tensor(0.5890, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  45  loss:  tensor(0.5579, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  50  loss:  tensor(0.6193, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  55  loss:  tensor(0.5834, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  60  loss:  tensor(0.6133, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  65  loss:  tensor(0.5322, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  70  loss:  tensor(0.6093, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  75  loss:  tensor(0.6117, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  80  loss:  tensor(0.5723, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  85  loss:  tensor(0.5945, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  90  loss:  tensor(0.5892, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  95  loss:  tensor(0.5626, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  100  loss:  tensor(0.6165, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  105  loss:  tensor(0.6093, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  110  loss:  tensor(0.5758, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  115  loss:  tensor(0.5627, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  120  loss:  tensor(0.5625, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  125  loss:  tensor(0.5976, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  130  loss:  tensor(0.5813, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  135  loss:  tensor(0.5560, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  140  loss:  tensor(0.5512, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  145  loss:  tensor(0.6960, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  150  loss:  tensor(0.5983, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  155  loss:  tensor(0.6144, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  160  loss:  tensor(0.5987, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  165  loss:  tensor(0.5722, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  170  loss:  tensor(0.5964, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  175  loss:  tensor(0.5847, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  180  loss:  tensor(0.6040, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  185  loss:  tensor(0.6500, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  190  loss:  tensor(0.5679, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  195  loss:  tensor(0.6141, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  200  loss:  tensor(0.5917, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  205  loss:  tensor(0.5167, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  210  loss:  tensor(0.5563, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  215  loss:  tensor(0.6027, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  220  loss:  tensor(0.5825, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  225  loss:  tensor(0.6002, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  230  loss:  tensor(0.5589, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  235  loss:  tensor(0.6392, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  240  loss:  tensor(0.6135, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  245  loss:  tensor(0.5188, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  250  loss:  tensor(0.6030, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  255  loss:  tensor(0.5708, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  260  loss:  tensor(0.5237, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  265  loss:  tensor(0.5918, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  batch:  270  loss:  tensor(0.5891, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  12  trainset auc:  0.89  dev set auc:  0.89\n",
            "epoch:  13  batch:  0  loss:  tensor(0.6152, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  5  loss:  tensor(0.6076, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  10  loss:  tensor(0.5938, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  15  loss:  tensor(0.5897, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  20  loss:  tensor(0.5797, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  25  loss:  tensor(0.5899, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  30  loss:  tensor(0.5838, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  35  loss:  tensor(0.5789, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  40  loss:  tensor(0.6105, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  45  loss:  tensor(0.5714, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  50  loss:  tensor(0.5318, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  55  loss:  tensor(0.6104, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  60  loss:  tensor(0.5726, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  65  loss:  tensor(0.6134, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  70  loss:  tensor(0.6012, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  75  loss:  tensor(0.5429, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  80  loss:  tensor(0.5454, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  85  loss:  tensor(0.5777, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  90  loss:  tensor(0.5916, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  95  loss:  tensor(0.5960, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  100  loss:  tensor(0.6257, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  105  loss:  tensor(0.5869, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  110  loss:  tensor(0.5557, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  115  loss:  tensor(0.5397, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  120  loss:  tensor(0.5677, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  125  loss:  tensor(0.5668, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  130  loss:  tensor(0.5640, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  135  loss:  tensor(0.5791, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  140  loss:  tensor(0.6069, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  145  loss:  tensor(0.5918, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  150  loss:  tensor(0.5844, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  155  loss:  tensor(0.5461, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  160  loss:  tensor(0.6160, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  165  loss:  tensor(0.6076, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  170  loss:  tensor(0.6246, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  175  loss:  tensor(0.5414, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  180  loss:  tensor(0.5781, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  185  loss:  tensor(0.5845, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  190  loss:  tensor(0.5712, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  195  loss:  tensor(0.5453, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  200  loss:  tensor(0.6001, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  205  loss:  tensor(0.5888, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  210  loss:  tensor(0.5798, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  215  loss:  tensor(0.6129, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  220  loss:  tensor(0.5560, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  225  loss:  tensor(0.5934, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  230  loss:  tensor(0.5530, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  235  loss:  tensor(0.5917, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  240  loss:  tensor(0.6393, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  245  loss:  tensor(0.6319, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  250  loss:  tensor(0.5986, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  255  loss:  tensor(0.6057, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  260  loss:  tensor(0.6155, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  265  loss:  tensor(0.5542, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  batch:  270  loss:  tensor(0.6296, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  13  trainset auc:  0.92  dev set auc:  0.91\n",
            "epoch:  14  batch:  0  loss:  tensor(0.6333, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  5  loss:  tensor(0.6047, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  10  loss:  tensor(0.5828, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  15  loss:  tensor(0.5516, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  20  loss:  tensor(0.5643, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  25  loss:  tensor(0.6136, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  30  loss:  tensor(0.5165, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  35  loss:  tensor(0.5732, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  40  loss:  tensor(0.5343, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  45  loss:  tensor(0.5903, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  50  loss:  tensor(0.5559, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  55  loss:  tensor(0.5805, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  60  loss:  tensor(0.5411, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  65  loss:  tensor(0.5793, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  70  loss:  tensor(0.5854, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  75  loss:  tensor(0.5368, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  80  loss:  tensor(0.6007, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  85  loss:  tensor(0.5347, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  90  loss:  tensor(0.6407, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  95  loss:  tensor(0.6024, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  100  loss:  tensor(0.5782, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  105  loss:  tensor(0.5652, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  110  loss:  tensor(0.6029, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  115  loss:  tensor(0.5548, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  120  loss:  tensor(0.5796, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  125  loss:  tensor(0.5890, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  130  loss:  tensor(0.5389, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  135  loss:  tensor(0.5301, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  140  loss:  tensor(0.5486, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  145  loss:  tensor(0.5859, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  150  loss:  tensor(0.4984, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  155  loss:  tensor(0.5627, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  160  loss:  tensor(0.6223, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  165  loss:  tensor(0.5732, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  170  loss:  tensor(0.5830, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  175  loss:  tensor(0.5754, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  180  loss:  tensor(0.5595, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  185  loss:  tensor(0.5767, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  190  loss:  tensor(0.5709, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  195  loss:  tensor(0.6001, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  200  loss:  tensor(0.6179, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  205  loss:  tensor(0.6055, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  210  loss:  tensor(0.5693, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  215  loss:  tensor(0.5823, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  220  loss:  tensor(0.5953, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  225  loss:  tensor(0.5535, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  230  loss:  tensor(0.6063, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  235  loss:  tensor(0.5710, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  240  loss:  tensor(0.5740, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  245  loss:  tensor(0.5583, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  250  loss:  tensor(0.6069, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  255  loss:  tensor(0.5991, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  260  loss:  tensor(0.5809, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  265  loss:  tensor(0.6157, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  batch:  270  loss:  tensor(0.6072, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  14  trainset auc:  0.89  dev set auc:  0.95\n",
            "epoch:  15  batch:  0  loss:  tensor(0.5955, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  5  loss:  tensor(0.6021, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  10  loss:  tensor(0.5790, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  15  loss:  tensor(0.5811, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  20  loss:  tensor(0.5576, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  25  loss:  tensor(0.5806, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  30  loss:  tensor(0.5879, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  35  loss:  tensor(0.5955, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  40  loss:  tensor(0.5884, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  45  loss:  tensor(0.5801, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  50  loss:  tensor(0.5666, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  55  loss:  tensor(0.5612, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  60  loss:  tensor(0.5830, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  65  loss:  tensor(0.5786, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  70  loss:  tensor(0.5400, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  75  loss:  tensor(0.5664, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  80  loss:  tensor(0.5828, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  85  loss:  tensor(0.5915, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  90  loss:  tensor(0.5915, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  95  loss:  tensor(0.5951, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  100  loss:  tensor(0.6020, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  105  loss:  tensor(0.6175, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  110  loss:  tensor(0.5816, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  115  loss:  tensor(0.6608, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  120  loss:  tensor(0.5816, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  125  loss:  tensor(0.6110, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  130  loss:  tensor(0.5463, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  135  loss:  tensor(0.5682, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  140  loss:  tensor(0.6337, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  145  loss:  tensor(0.5569, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  150  loss:  tensor(0.5279, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  155  loss:  tensor(0.5794, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  160  loss:  tensor(0.6043, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  165  loss:  tensor(0.5448, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  170  loss:  tensor(0.6383, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  175  loss:  tensor(0.6292, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  180  loss:  tensor(0.5350, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  185  loss:  tensor(0.5722, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  190  loss:  tensor(0.5793, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  195  loss:  tensor(0.6068, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  200  loss:  tensor(0.5932, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  205  loss:  tensor(0.5717, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  210  loss:  tensor(0.5881, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  215  loss:  tensor(0.6012, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  220  loss:  tensor(0.6171, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  225  loss:  tensor(0.5700, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  230  loss:  tensor(0.5747, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  235  loss:  tensor(0.5986, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  240  loss:  tensor(0.5720, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  245  loss:  tensor(0.5528, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  250  loss:  tensor(0.5703, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  255  loss:  tensor(0.5638, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  260  loss:  tensor(0.5657, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  265  loss:  tensor(0.5521, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  batch:  270  loss:  tensor(0.5804, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  15  trainset auc:  0.91  dev set auc:  0.93\n",
            "epoch:  16  batch:  0  loss:  tensor(0.5043, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  5  loss:  tensor(0.5470, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  10  loss:  tensor(0.5801, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  15  loss:  tensor(0.6064, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  20  loss:  tensor(0.5999, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  25  loss:  tensor(0.5838, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  30  loss:  tensor(0.5444, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  35  loss:  tensor(0.5682, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  40  loss:  tensor(0.6330, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  45  loss:  tensor(0.5508, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  50  loss:  tensor(0.5783, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  55  loss:  tensor(0.6154, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  60  loss:  tensor(0.5754, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  65  loss:  tensor(0.5155, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  70  loss:  tensor(0.5315, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  75  loss:  tensor(0.5814, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  80  loss:  tensor(0.5971, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  85  loss:  tensor(0.5163, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  90  loss:  tensor(0.5556, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  95  loss:  tensor(0.5667, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  100  loss:  tensor(0.5592, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  105  loss:  tensor(0.5868, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  110  loss:  tensor(0.5612, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  115  loss:  tensor(0.5582, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  120  loss:  tensor(0.6511, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  125  loss:  tensor(0.5755, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  130  loss:  tensor(0.5847, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  135  loss:  tensor(0.5813, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  140  loss:  tensor(0.6118, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  145  loss:  tensor(0.5328, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  150  loss:  tensor(0.5473, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  155  loss:  tensor(0.5479, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  160  loss:  tensor(0.5745, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  165  loss:  tensor(0.5398, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  170  loss:  tensor(0.5369, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  175  loss:  tensor(0.5727, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  180  loss:  tensor(0.5977, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  185  loss:  tensor(0.6151, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  190  loss:  tensor(0.5791, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  195  loss:  tensor(0.5381, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  200  loss:  tensor(0.5972, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  205  loss:  tensor(0.5376, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  210  loss:  tensor(0.5419, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  215  loss:  tensor(0.5828, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  220  loss:  tensor(0.6111, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  225  loss:  tensor(0.5213, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  230  loss:  tensor(0.6317, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  235  loss:  tensor(0.5559, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  240  loss:  tensor(0.5558, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  245  loss:  tensor(0.6109, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  250  loss:  tensor(0.5621, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  255  loss:  tensor(0.5524, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  260  loss:  tensor(0.5321, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  265  loss:  tensor(0.5372, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  batch:  270  loss:  tensor(0.5939, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "epoch:  16  trainset auc:  0.9  dev set auc:  0.88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GSkbCgo8ZShT",
        "colab_type": "code",
        "outputId": "cdb03845-e318-4973-d50f-e586bbeca11a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "epochs = np.array(range(len(train_set_auc))) + 1\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.plot(range(len(train_set_auc)), train_set_auc, color = \"orange\", label='train')\n",
        "plt.plot(range(len(dev_set_auc)), dev_set_auc, color = \"blue\", label='dev')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Pretrained Embeddings LSTM')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('AUC')\n",
        "# plt.ylim((-0.05, 1.05))\n",
        "plt.show()\n",
        "\n",
        "fn = 'Pretrained Embeddings LSTM.png'\n",
        "plt.savefig(fn)\n",
        "from google.colab import files\n",
        "files.download(fn) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARYAAAEVCAYAAAAl2crhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "OVkjs8_cNjag",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How does it go on the test data?"
      ]
    },
    {
      "metadata": {
        "id": "GUbFJU4QJ7E6",
        "colab_type": "code",
        "outputId": "7c13af93-9e8b-4909-8714-50dbfdf0f95f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "# net_1 = LSTM(input_dim=100, hidden_dim=50, batch_size=1, output_dim=1, num_layers=1).cuda()\n",
        "\n",
        "# net_1.load_state_dict(torch.load('pretrained_embeddings_sentence_average_20'))\n",
        "\n",
        "get_dataset_auc(imdb_data_sentence['test'], net, len(imdb_data_sentence['test']['sentences']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9091345536000001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "_Mjbt9wJpFHY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict_review(review, net):\n",
        "    review = review.split(' ')\n",
        "    se = sentence_embedding(review, word_vectors).cuda()\n",
        "\n",
        "    net.hidden = net.init_hidden()\n",
        "    pred = net(se)\n",
        "    \n",
        "    if pred > 0.5:\n",
        "        print('a positive review')\n",
        "    else:\n",
        "        print('a negative review')\n",
        "    \n",
        "    return pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r9ppqE7S9kZM",
        "colab_type": "code",
        "outputId": "e1d2c1a1-197c-44d9-9be7-8e838cfae7e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "review = 'It was good.'\n",
        "predict_review(review, net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a positive review\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6294], device='cuda:0', grad_fn=<ViewBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "rgFQQOrQnNTC",
        "colab_type": "code",
        "outputId": "53cc7b74-4ed5-4b8e-d84d-a1dcc7fc7e03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "review = 'It was not good.'\n",
        "predict_review(review, net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a negative review\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.3356], device='cuda:0', grad_fn=<ViewBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "metadata": {
        "id": "exQBe0AtnQAZ",
        "colab_type": "code",
        "outputId": "ca091456-8049-4789-fd3a-456e4724962d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "review = 'That was the best movie I have ever seen in my entire life.'\n",
        "predict_review(review, net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a positive review\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.8363], device='cuda:0', grad_fn=<ViewBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "w2jvX-ZHnYDr",
        "colab_type": "code",
        "outputId": "362fdf69-a3b5-4f48-d888-463beeb46500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "review = 'That film sucked! What a waste of time.'\n",
        "predict_review(review, net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a negative review\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0014], device='cuda:0', grad_fn=<ViewBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "metadata": {
        "id": "4TLSHIZJoOFQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compare predictions to the training data"
      ]
    },
    {
      "metadata": {
        "id": "gzen5mRk9ki4",
        "colab_type": "code",
        "outputId": "0ae7e554-5754-4ce0-8fe9-5e9640df99de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "step += 1\n",
        "text = imdb_data_sentence['train']['sentences'][step].cuda()\n",
        "l = torch.Tensor([imdb_data_sentence['train']['labels'][step]]).cuda()\n",
        "\n",
        "net.hidden = net.init_hidden()\n",
        "pred = net(text)  \n",
        "print(pred)\n",
        "print(l)\n",
        "print(imdb_data['train'].examples[step].text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.9993], device='cuda:0', grad_fn=<ViewBackward>)\n",
            "tensor([1.], device='cuda:0')\n",
            "['I', 'watched', 'the', 'un', '-', 'aired', 'episodes', 'online', 'and', 'I', 'was', 'so', 'sad', 'that', 'the', 'show', 'wo', \"n't\", 'be', 'back', '.', 'It', 'had', 'the', 'best', 'cast', 'of', 'mature', ',', 'talented', 'actors', 'and', 'an', 'amazing', 'chemistry', '.', 'It', 'seemed', 'like', 'all', 'the', 'actors', 'are', 'personal', 'friends', 'in', 'real', 'life', '.', 'Towards', 'the', 'end', 'the', 'show', 'became', 'engaging', ',', 'sexy', 'and', 'highly', 'watchable', '.', 'Of', 'course', ',', 'some', 'of', 'the', 'story', 'lines', 'are', 'not', 'realistic', ',', 'so', 'what', '...', 'The', 'characters', 'are', 'all', 'likable', 'and', 'you', 'root', 'for', 'them', '.', 'The', 'show', 'reminded', 'me', 'a', 'cross', 'between', '2', 'other', 'favorites', ':', '\"', 'Sex', 'and', 'the', 'City', '\"', 'and', '\"', 'Felicity', '\"', '.', 'Big', 'kudos', 'to', 'all', 'the', 'cast', '.', 'Note', 'to', 'ABC', 'execs', ':', 'Nielsen', 'ratings', 'reports', 'do', 'not', 'show', 'you', 'true', 'results', '.', 'The', 'show', 'audience', 'will', 'mostly', 'record', 'it', '.', 'I', \"'ve\", 'been', 'very', 'disappointed', 'with', 'major', 'networks', 'for', 'flooding', 'us', 'with', 'reality', '-', 'TV', 'or', 'teenage', 'oriented', 'shows', '.', 'Why', 'to', 'get', 'a', 'mature', ',', 'thoughtful', ',', 'well', '-', 'acted', 'material', 'we', 'have', 'to', 'switch', 'to', 'HBO', 'or', 'FX', '?', 'I', 'can', 'only', 'thank', 'the', 'network', 'for', 'putting', 'the', 'rest', 'of', 'the', 'episodes', 'online', '.', 'The', 'new', 'stream', 'media', 'will', 'gain', 'more', 'and', 'more', 'popularity', 'among', 'viewers', '.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "96o8Miw7o8Nw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KwFx-B8So8MI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}